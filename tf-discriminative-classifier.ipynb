{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_json_to_review_and_rating(json_text):\n",
    "    review_dict = json.loads(json_text)    \n",
    "    return review_dict['reviewText'], review_dict['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviews_and_ratings(reviews_filepath):\n",
    "    review_texts = list()\n",
    "    ratings = list()\n",
    "    with open(reviews_filepath) as reviews_file:\n",
    "        for line in reviews_file:\n",
    "            review_text, rating = convert_json_to_review_and_rating(line)\n",
    "            review_texts.append(review_text)\n",
    "            ratings.append(int(rating))\n",
    "            \n",
    "    return review_texts, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texts_to_indexed_word_sequences(review_texts):\n",
    "    word_indices = dict()\n",
    "    indexed_sequences = list()\n",
    "    word_index = 1\n",
    "    \n",
    "    for review_text in review_texts:\n",
    "        tokens = word_tokenize(review_text)\n",
    "        indexed_sequence = list()\n",
    "        for token in tokens:\n",
    "            if token not in word_indices:\n",
    "                word_indices[token] = word_index\n",
    "                indexed_sequence.append(word_index)\n",
    "                word_index += 1\n",
    "            else:\n",
    "                indexed_sequence.append(word_indices[token])\n",
    "        indexed_sequences.append(np.asarray(indexed_sequence))\n",
    "        \n",
    "    return word_indices, indexed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_filepath = \"/home/v2john/datasets/amazon/reviews_electronics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_texts, ratings = get_reviews_and_ratings(reviews_filepath)\n",
    "print(len(review_texts), len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_indices, indexed_sequences = texts_to_indexed_word_sequences(review_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_indices)\n",
    "print(\"VOCAB_SIZE: \", VOCAB_SIZE)\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "print(\"EMBEDDING_SIZE: \", EMBEDDING_SIZE)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = int(median([len(sequence) for sequence in indexed_sequences]))\n",
    "print(\"MAX_SEQUENCE_LENGTH: \", MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "NUM_CLASSES = len(set(ratings))\n",
    "print(\"NUM_CLASSES: \", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_indexed_sequences(indexed_sequences, max_sequence_length):\n",
    "    new_indexed_sequences = list()\n",
    "    for sequence in indexed_sequences:\n",
    "        if len(sequence) >= max_sequence_length:\n",
    "            new_indexed_sequences.append(sequence[:max_sequence_length])\n",
    "        else:\n",
    "            shortfall = max_sequence_length - len(sequence)\n",
    "            new_indexed_sequences.append(\n",
    "                np.pad(sequence, (0, shortfall), 'constant', \n",
    "                       constant_values=(0, 0)))\n",
    "    return np.asarray(new_indexed_sequences)\n",
    "\n",
    "def convert_labels_to_logits(ratings, num_classes):\n",
    "    one_hot_ratings = list()\n",
    "    for rating in ratings:\n",
    "        one_hot_rating = np.zeros(num_classes)\n",
    "        one_hot_rating[rating - 1] = 1\n",
    "        one_hot_ratings.append(one_hot_rating)\n",
    "        \n",
    "    return np.asarray(one_hot_ratings)\n",
    "\n",
    "def tensorize_sequences_and_labels(indexed_sequences, ratings, max_sequence_length, num_classes):\n",
    "    return pad_indexed_sequences(indexed_sequences, max_sequence_length), \\\n",
    "        convert_labels_to_logits(ratings, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_sequences, labels = tensorize_sequences_and_labels(\n",
    "    indexed_sequences, ratings, MAX_SEQUENCE_LENGTH, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_sequences.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    \n",
    "    input_x = tf.placeholder(\n",
    "        tf.int32, [None, MAX_SEQUENCE_LENGTH], name=\"input_x\")\n",
    "    input_y = tf.placeholder(\n",
    "        tf.int32, [None, NUM_CLASSES], name=\"input_y\")\n",
    "\n",
    "    word_embeddings = tf.get_variable(\n",
    "        shape=[VOCAB_SIZE, EMBEDDING_SIZE], name=\"word_embeddings\", \n",
    "        dtype=tf.float32)\n",
    "    print(\"word_embeddings.shape: \", word_embeddings.shape)\n",
    "    \n",
    "    embedded_sequence = tf.nn.embedding_lookup(\n",
    "        word_embeddings, input_x, name=\"embedded_sequence\")\n",
    "    print(\"embedded_sequence.shape: \", embedded_sequence.shape)\n",
    "    \n",
    "    conv_1 = tf.layers.conv1d(\n",
    "        inputs=embedded_sequence, filters=300, kernel_size=3, name=\"conv_1\")\n",
    "    print(\"conv_1.shape: \", conv_1.shape)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv_1, pool_size=2, strides=2, name=\"max_pool_1\")\n",
    "    print(\"max_pool_1.shape: \", max_pool_1.shape)\n",
    "    \n",
    "    conv_2 = tf.layers.conv1d(\n",
    "        inputs=max_pool_1, filters=150, kernel_size=3, name=\"conv_2\")\n",
    "    print(\"conv_2.shape: \", conv_2.shape)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv_2, pool_size=2, strides=2, name=\"max_pool_2\")\n",
    "    print(\"max_pool_2.shape: \", max_pool_2.shape)\n",
    "    \n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(128)\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(\n",
    "        cell=lstm_cell_1, inputs=max_pool_2, dtype=tf.float32)\n",
    "    print(\"current_state.h.shape: \", current_state.h.shape)\n",
    "    \n",
    "    dense_1 = tf.layers.dense(\n",
    "        inputs=current_state.h, units=NUM_CLASSES, name=\"dense_1\")\n",
    "    print(\"dense_1.shape: \", dense_1.shape)\n",
    "    \n",
    "    softmax_output = tf.nn.softmax(dense_1, name=\"softmax\")\n",
    "    print(\"softmax_output.shape: \", softmax_output.shape)\n",
    "    \n",
    "    one_hot_label = tf.one_hot(\n",
    "        indices=input_y-1, depth=1, on_value=1, off_value=0,\n",
    "        name=\"one_hot_label\")\n",
    "    one_hot_label = tf.reshape(one_hot_label, tf.shape(softmax_output))\n",
    "    print(\"one_hot_label.shape: \", one_hot_label.shape)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        one_hot_label, softmax_output)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_reporting_interval = 50\n",
    "\n",
    "with tf.Session(graph=graph_1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(0, 1000):\n",
    "        _, l = \\\n",
    "            sess.run([optimizer, loss], \n",
    "                feed_dict={\n",
    "                    input_x: indexed_sequences[i*100: (i+1)*100],\n",
    "                    input_y: labels[i*100: (i+1)*100]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        if (i % epoch_reporting_interval == 0):\n",
    "            print(\"Training epoch: \", i, \", Loss: \", l)\n",
    "            \n",
    "    final_predictions = \\\n",
    "        sess.run(\n",
    "            [softmax_output], \n",
    "            feed_dict={\n",
    "                input_x: indexed_sequences, \n",
    "                input_y: labels\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    for prediction in final_predictions:\n",
    "        print(sess.run(tf.contrib.seq2seq.hardmax(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
