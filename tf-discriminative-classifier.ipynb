{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_json_to_review_and_rating(json_text):\n",
    "    review_dict = json.loads(json_text)    \n",
    "    return review_dict['reviewText'], review_dict['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviews_and_ratings(reviews_filepath):\n",
    "    review_texts = list()\n",
    "    ratings = list()\n",
    "    with open(reviews_filepath) as reviews_file:\n",
    "        for line in reviews_file:\n",
    "            review_text, rating = convert_json_to_review_and_rating(line)\n",
    "            review_texts.append(review_text)\n",
    "            ratings.append(int(rating))\n",
    "            \n",
    "    return review_texts, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texts_to_indexed_word_sequences(review_texts):\n",
    "    word_indices = dict()\n",
    "    indexed_sequences = list()\n",
    "    word_index = 1\n",
    "    \n",
    "    for review_text in review_texts:\n",
    "        tokens = word_tokenize(review_text)\n",
    "        indexed_sequence = list()\n",
    "        for token in tokens:\n",
    "            if token not in word_indices:\n",
    "                word_indices[token] = word_index\n",
    "                indexed_sequence.append(word_index)\n",
    "                word_index += 1\n",
    "            else:\n",
    "                indexed_sequence.append(word_indices[token])\n",
    "        indexed_sequences.append(np.asarray(indexed_sequence))\n",
    "        \n",
    "    return word_indices, indexed_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_filepath = \"/home/v2john/datasets/amazon/reviews_electronics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "review_texts, ratings = get_reviews_and_ratings(reviews_filepath)\n",
    "print(len(review_texts), len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_indices, indexed_sequences = texts_to_indexed_word_sequences(review_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE:  46265\n",
      "EMBEDDING_SIZE:  300\n",
      "MAX_SEQUENCE_LENGTH:  77\n",
      "NUM_CLASSES:  5\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_indices)\n",
    "print(\"VOCAB_SIZE: \", VOCAB_SIZE)\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "print(\"EMBEDDING_SIZE: \", EMBEDDING_SIZE)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = int(median([len(sequence) for sequence in indexed_sequences]))\n",
    "print(\"MAX_SEQUENCE_LENGTH: \", MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "NUM_CLASSES = len(set(ratings))\n",
    "print(\"NUM_CLASSES: \", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_indexed_sequences(indexed_sequences, max_sequence_length):\n",
    "    new_indexed_sequences = list()\n",
    "    for sequence in indexed_sequences:\n",
    "        if len(sequence) >= max_sequence_length:\n",
    "            new_indexed_sequences.append(sequence[:max_sequence_length])\n",
    "        else:\n",
    "            shortfall = max_sequence_length - len(sequence)\n",
    "            new_indexed_sequences.append(\n",
    "                np.pad(sequence, (0, shortfall), 'constant', \n",
    "                       constant_values=(0, 0)))\n",
    "    return np.asarray(new_indexed_sequences)\n",
    "\n",
    "def convert_labels_to_logits(ratings, num_classes):\n",
    "    one_hot_ratings = list()\n",
    "    for rating in ratings:\n",
    "        one_hot_rating = np.zeros(num_classes)\n",
    "        one_hot_rating[rating - 1] = 1\n",
    "        one_hot_ratings.append(one_hot_rating)\n",
    "        \n",
    "    return np.asarray(one_hot_ratings)\n",
    "\n",
    "def tensorize_sequences_and_labels(indexed_sequences, ratings, max_sequence_length, num_classes):\n",
    "    return pad_indexed_sequences(indexed_sequences, max_sequence_length), \\\n",
    "        convert_labels_to_logits(ratings, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_sequences, labels = tensorize_sequences_and_labels(\n",
    "    indexed_sequences, ratings, MAX_SEQUENCE_LENGTH, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 77), (10000, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sequences.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings:  <tf.Variable 'word_embeddings:0' shape=(46265, 300) dtype=float32_ref>\n",
      "embedded_sequence:  Tensor(\"embedded_sequence:0\", shape=(?, 77, 300), dtype=float32)\n",
      "conv_1:  Tensor(\"conv_1/BiasAdd:0\", shape=(?, 75, 64), dtype=float32)\n",
      "max_pool_1:  Tensor(\"max_pool_1/Squeeze:0\", shape=(?, 37, 64), dtype=float32)\n",
      "conv_2:  Tensor(\"conv_2/BiasAdd:0\", shape=(?, 35, 128), dtype=float32)\n",
      "max_pool_2:  Tensor(\"max_pool_2/Squeeze:0\", shape=(?, 17, 128), dtype=float32)\n",
      "fw_lstm_output:  Tensor(\"bidirectional_rnn/fw/fw/while/Exit_3:0\", shape=(?, 32), dtype=float32)\n",
      "bw_lstm_output:  Tensor(\"bidirectional_rnn/bw/bw/while/Exit_3:0\", shape=(?, 32), dtype=float32)\n",
      "lstm_output:  Tensor(\"concat:0\", shape=(?, 64), dtype=float32)\n",
      "dense_1.shape:  (?, 5)\n",
      "softmax_output.shape:  (?, 5)\n",
      "one_hot_label.shape:  (?, 5)\n"
     ]
    }
   ],
   "source": [
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    \n",
    "    input_x = tf.placeholder(\n",
    "        tf.int32, [None, MAX_SEQUENCE_LENGTH], name=\"input_x\")\n",
    "    input_y = tf.placeholder(\n",
    "        tf.int32, [None, NUM_CLASSES], name=\"input_y\")\n",
    "\n",
    "    word_embeddings = tf.get_variable(\n",
    "        shape=[VOCAB_SIZE, EMBEDDING_SIZE], name=\"word_embeddings\", \n",
    "        dtype=tf.float32)\n",
    "    print(\"word_embeddings: \", word_embeddings)\n",
    "    \n",
    "    embedded_sequence = tf.nn.embedding_lookup(\n",
    "        word_embeddings, input_x, name=\"embedded_sequence\")\n",
    "    print(\"embedded_sequence: \", embedded_sequence)\n",
    "    \n",
    "    conv_1 = tf.layers.conv1d(\n",
    "        inputs=embedded_sequence, filters=64, kernel_size=3, name=\"conv_1\")\n",
    "    print(\"conv_1: \", conv_1)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(\n",
    "        inputs=conv_1, pool_size=2, strides=2, name=\"max_pool_1\")\n",
    "    print(\"max_pool_1: \", max_pool_1)\n",
    "    \n",
    "    conv_2 = tf.layers.conv1d(\n",
    "        inputs=max_pool_1, filters=128, kernel_size=3, name=\"conv_2\")\n",
    "    print(\"conv_2: \", conv_2)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(\n",
    "        inputs=conv_2, pool_size=2, strides=2, name=\"max_pool_2\")\n",
    "    print(\"max_pool_2: \", max_pool_2)\n",
    "    \n",
    "    lstm_cell_fw = tf.contrib.rnn.BasicLSTMCell(32)\n",
    "    lstm_cell_bw = tf.contrib.rnn.BasicLSTMCell(32)\n",
    "    \n",
    "    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=lstm_cell_fw, cell_bw=lstm_cell_bw, inputs=max_pool_2, \n",
    "        dtype=tf.float32)\n",
    "    print(\"fw_lstm_output: \", output_states[0].h)\n",
    "    print(\"bw_lstm_output: \", output_states[1].h)\n",
    "    \n",
    "    lstm_output = tf.concat([output_states[0].h, output_states[1].h], axis=1)\n",
    "    print(\"lstm_output: \", lstm_output)\n",
    "    \n",
    "    dense_1 = tf.layers.dense(\n",
    "        inputs=lstm_output, units=NUM_CLASSES, name=\"dense_1\")\n",
    "    print(\"dense_1.shape: \", dense_1.shape)\n",
    "    \n",
    "    softmax_output = tf.nn.softmax(dense_1, name=\"softmax\")\n",
    "    print(\"softmax_output.shape: \", softmax_output.shape)\n",
    "    \n",
    "    one_hot_label = tf.one_hot(\n",
    "        indices=input_y-1, depth=1, on_value=1, off_value=0,\n",
    "        name=\"one_hot_label\")\n",
    "    one_hot_label = tf.reshape(one_hot_label, tf.shape(softmax_output))\n",
    "    print(\"one_hot_label.shape: \", one_hot_label.shape)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        one_hot_label, softmax_output)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch:  10 , Loss:  1.25751\n",
      "Training epoch:  20 , Loss:  1.19797\n",
      "Training epoch:  30 , Loss:  1.10118\n",
      "Training epoch:  40 , Loss:  1.04328\n",
      "Training epoch:  50 , Loss:  0.976438\n",
      "Training epoch:  60 , Loss:  0.966803\n",
      "Training epoch:  70 , Loss:  0.965322\n",
      "Training epoch:  80 , Loss:  0.975027\n",
      "Training epoch:  90 , Loss:  0.962458\n",
      "Training epoch:  100 , Loss:  0.953298\n",
      "Training epoch:  110 , Loss:  0.952695\n",
      "Training epoch:  120 , Loss:  0.942155\n",
      "Training epoch:  130 , Loss:  0.942023\n",
      "Training epoch:  140 , Loss:  0.983687\n",
      "Training epoch:  150 , Loss:  0.942833\n",
      "Training epoch:  160 , Loss:  0.942756\n",
      "Training epoch:  170 , Loss:  0.946515\n",
      "Training epoch:  180 , Loss:  0.943674\n",
      "Training epoch:  190 , Loss:  0.952881\n",
      "Training epoch:  200 , Loss:  0.934023\n",
      "Training epoch:  210 , Loss:  0.933716\n",
      "Training epoch:  220 , Loss:  0.933452\n",
      "Training epoch:  230 , Loss:  0.934012\n",
      "Training epoch:  240 , Loss:  0.949581\n",
      "Training epoch:  250 , Loss:  0.934911\n",
      "Training epoch:  260 , Loss:  0.934851\n",
      "Training epoch:  270 , Loss:  0.9343\n",
      "Training epoch:  280 , Loss:  0.938061\n",
      "Training epoch:  290 , Loss:  0.934827\n",
      "Training epoch:  300 , Loss:  0.934245\n",
      "Training epoch:  310 , Loss:  0.934304\n",
      "Training epoch:  320 , Loss:  0.966794\n",
      "Training epoch:  330 , Loss:  0.924764\n",
      "Training epoch:  340 , Loss:  0.924916\n",
      "Training epoch:  350 , Loss:  0.924121\n",
      "Training epoch:  360 , Loss:  0.924811\n",
      "Training epoch:  370 , Loss:  0.924096\n",
      "Training epoch:  380 , Loss:  0.937389\n",
      "Training epoch:  390 , Loss:  0.927777\n",
      "Training epoch:  400 , Loss:  0.926955\n",
      "Training epoch:  410 , Loss:  0.925398\n",
      "Training epoch:  420 , Loss:  0.935438\n",
      "Training epoch:  430 , Loss:  0.926407\n",
      "Training epoch:  440 , Loss:  0.925163\n",
      "Training epoch:  450 , Loss:  0.92461\n",
      "Training epoch:  460 , Loss:  0.938219\n",
      "Training epoch:  470 , Loss:  0.924855\n",
      "Training epoch:  480 , Loss:  0.924929\n",
      "Training epoch:  490 , Loss:  0.925134\n",
      "Training epoch:  500 , Loss:  0.924848\n",
      "Training epoch:  510 , Loss:  0.93087\n",
      "Training epoch:  520 , Loss:  0.936892\n",
      "Training epoch:  530 , Loss:  0.926635\n",
      "Training epoch:  540 , Loss:  0.927838\n",
      "Training epoch:  550 , Loss:  0.946088\n",
      "Training epoch:  560 , Loss:  0.936418\n",
      "Training epoch:  570 , Loss:  0.924912\n",
      "Training epoch:  580 , Loss:  0.924526\n",
      "Training epoch:  590 , Loss:  0.935868\n",
      "Training epoch:  600 , Loss:  0.924311\n",
      "Training epoch:  610 , Loss:  0.923833\n",
      "Training epoch:  620 , Loss:  0.925764\n",
      "Training epoch:  630 , Loss:  0.928367\n",
      "Training epoch:  640 , Loss:  0.927222\n",
      "Training epoch:  650 , Loss:  0.929901\n",
      "Training epoch:  660 , Loss:  0.932832\n",
      "Training epoch:  670 , Loss:  0.936204\n",
      "Training epoch:  680 , Loss:  0.934547\n",
      "Training epoch:  690 , Loss:  0.924614\n",
      "Training epoch:  700 , Loss:  0.924338\n",
      "Training epoch:  710 , Loss:  0.925721\n",
      "Training epoch:  720 , Loss:  0.925192\n",
      "Training epoch:  730 , Loss:  0.930454\n",
      "Training epoch:  740 , Loss:  0.925319\n",
      "Training epoch:  750 , Loss:  0.924928\n",
      "Training epoch:  760 , Loss:  0.942972\n",
      "Training epoch:  770 , Loss:  0.93471\n",
      "Training epoch:  780 , Loss:  0.932388\n",
      "Training epoch:  790 , Loss:  0.93416\n",
      "Training epoch:  800 , Loss:  0.949418\n",
      "Training epoch:  810 , Loss:  0.924289\n",
      "Training epoch:  820 , Loss:  0.925159\n",
      "Training epoch:  830 , Loss:  0.927605\n",
      "Training epoch:  840 , Loss:  0.930399\n",
      "Training epoch:  850 , Loss:  0.935438\n",
      "Training epoch:  860 , Loss:  0.941187\n",
      "Training epoch:  870 , Loss:  0.924763\n",
      "Training epoch:  880 , Loss:  0.924949\n",
      "Training epoch:  890 , Loss:  0.92478\n",
      "Training epoch:  900 , Loss:  0.925049\n",
      "Training epoch:  910 , Loss:  0.924875\n",
      "Training epoch:  920 , Loss:  0.925132\n",
      "Training epoch:  930 , Loss:  0.924878\n",
      "Training epoch:  940 , Loss:  0.924952\n",
      "Training epoch:  950 , Loss:  0.941432\n",
      "Training epoch:  960 , Loss:  0.926372\n",
      "Training epoch:  970 , Loss:  0.924793\n",
      "Training epoch:  980 , Loss:  0.934991\n",
      "Training epoch:  990 , Loss:  0.941831\n",
      "Training epoch:  1000 , Loss:  0.957101\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_1) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    epoch_reporting_interval = 10\n",
    "    batch_size = 100\n",
    "    \n",
    "    for current_epoch in range(1, 1001):\n",
    "        for batch_number in range(0, 100):\n",
    "            _, loss_var = sess.run(\n",
    "                [train_op, loss], \n",
    "                feed_dict={\n",
    "                    input_x: indexed_sequences[batch_number * batch_size : \n",
    "                                               (batch_number + 1) * batch_size],\n",
    "                    input_y: labels[batch_number * batch_size : \n",
    "                                    (batch_number + 1) * batch_size]\n",
    "                })\n",
    "\n",
    "        if (current_epoch % epoch_reporting_interval == 0):\n",
    "            print(\"Training epoch: \", current_epoch, \", Loss: \", loss_var)\n",
    "            \n",
    "    final_predictions = sess.run(\n",
    "        softmax_output, \n",
    "        feed_dict={\n",
    "            input_x: indexed_sequences, \n",
    "            input_y: labels\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "y_pred = list()\n",
    "for prediction in final_predictions:\n",
    "    y_label = np.argmax(prediction) + 1\n",
    "    y_pred.append(y_label)\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98489999999999989"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=ratings, y_pred=y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 556,    2,    5,    3,    6],\n",
       "       [   4,  439,    6,    0,    1],\n",
       "       [  10,    5,  772,   18,   17],\n",
       "       [   3,    2,   14, 2049,   27],\n",
       "       [   4,    2,    7,   15, 6033]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ratings, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
